{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e6ca89-674d-4327-90d4-db1a5920a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f04381e-896a-4ee4-913d-07b23e5ee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac12660-412e-44d8-aafe-b83a1f0f5eab",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "### 1.1 Prepare LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60720ddb-2a55-4b97-a0de-e15be24a57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "OLLAMA_HOST = 'localhost'\n",
    "OLLAMA_MODEL = 'mistral'\n",
    "llm = Ollama(model=OLLAMA_MODEL, base_url=\"http://\"+OLLAMA_HOST+\":11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01bb1371-01bf-40cb-b56a-58bc219ba532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    # To save costs, we use a local model.\n",
    "    # This will use a well-performing and fast default from Hugging Face.\n",
    "    # this model has dim of 384 https://huggingface.co/BAAI/bge-small-en\n",
    "    embed_model=\"local:BAAI/bge-small-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285d07a-8d12-4dee-b3cf-4fd1e1a863b5",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Graph Store\n",
    "`Neo4j` is supported as a graph store integration. You can persist, visualize, and query graphs using LlamaIndex and Neo4j. Furthermore, existing Neo4j graphs are directly supported using `text2cypher` and the `KnowledgeGraphQueryEngine`.\n",
    "\n",
    "If you’ve never used Neo4j before, you can download the desktop client [here](https://neo4j.com/download/).\n",
    "\n",
    "Once you open the client, create a new project and install the `apoc` integration. Full instructions here. Just click on your project, select `Plugins` on the left side menu, install APOC and restart your server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d15c05-009d-4677-b1a9-1da5ed67651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"neo4j\"\n",
    "password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "# Neo4j cloud has a generous free tier, so I use that instead of localhost\n",
    "url = \"bolt://localhost:7687\"\n",
    "# url = \"bolt+s://3b2530f1.databases.neo4j.io:7687\"\n",
    "database = \"neo4j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a2128b-9575-4a2b-bc0d-bfcdcde5a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores import Neo4jGraphStore\n",
    "\n",
    "graph_store = Neo4jGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    database=database,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751a5075-72d7-4fbd-99af-3ca75de8dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50bd077-5864-4fb7-8a25-793e9eaa69fd",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "### 2.1 Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a8e7d3-349b-4f37-8bcb-2ecebb819833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fcf49-16f5-47b5-a2a3-561fd25211ff",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to Graph\n",
    "reference:\n",
    "- [KnowledgeGraphIndex](https://docs.llamaindex.ai/en/stable/api_reference/indices/kg.html#llama_index.indices.knowledge_graph.KnowledgeGraphIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfd75ae9-d71a-4a6e-be19-ba38cbf9a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.75it/s]\n",
      "Processing nodes:   0%|                                                                                                                                               | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 37.97it/s]\u001b[A\n",
      "Processing nodes:   6%|████████▍                                                                                                                              | 1/16 [00:15<03:46, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 27.65it/s]\u001b[A\n",
      "Processing nodes:  12%|████████████████▉                                                                                                                      | 2/16 [00:29<03:28, 14.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 44.77it/s]\u001b[A\n",
      "Processing nodes:  19%|█████████████████████████▎                                                                                                             | 3/16 [00:41<02:56, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 62.60it/s]\u001b[A\n",
      "Processing nodes:  25%|█████████████████████████████████▊                                                                                                     | 4/16 [00:53<02:32, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 49.73it/s]\u001b[A\n",
      "Processing nodes:  31%|██████████████████████████████████████████▏                                                                                            | 5/16 [01:12<02:46, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 41.95it/s]\u001b[A\n",
      "Processing nodes:  38%|██████████████████████████████████████████████████▋                                                                                    | 6/16 [01:26<02:27, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 61.01it/s]\u001b[A\n",
      "Processing nodes:  44%|███████████████████████████████████████████████████████████                                                                            | 7/16 [01:39<02:08, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 172.88it/s]\u001b[A\n",
      "Processing nodes:  50%|███████████████████████████████████████████████████████████████████▌                                                                   | 8/16 [01:48<01:39, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 48.68it/s]\u001b[A\n",
      "Processing nodes:  56%|███████████████████████████████████████████████████████████████████████████▉                                                           | 9/16 [02:08<01:44, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 165.04it/s]\u001b[A\n",
      "Processing nodes:  62%|███████████████████████████████████████████████████████████████████████████████████▊                                                  | 10/16 [02:23<01:28, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 79.52it/s]\u001b[A\n",
      "Processing nodes:  69%|████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 11/16 [02:38<01:14, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 162.62it/s]\u001b[A\n",
      "Processing nodes:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 12/16 [02:49<00:55, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 80.65it/s]\u001b[A\n",
      "Processing nodes:  81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 13/16 [03:03<00:41, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.14it/s]\u001b[A\n",
      "Processing nodes:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 14/16 [03:21<00:30, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.02it/s]\u001b[A\n",
      "Processing nodes:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 15/16 [03:41<00:16, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 0it [00:00, ?it/s]\u001b[A\n",
      "Processing nodes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:52<00:00, 14.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex\n",
    "\n",
    "index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True,\n",
    "    show_progress=True\n",
    "    # max_object_length: int = 128,\n",
    "    \n",
    "    # to extract triplets, kg_triplet_extract_fn is used if not None,\n",
    "    # kg_triplet_extract_fn: Optional[Callable] = None, \n",
    "    # else, the LLM from the service context is used with the kg_triple_extract_template if not None else the default triplet extract prompt\n",
    "    # kg_triple_extract_template: Optional[BasePromptTemplate] = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44c81d-8dd9-4a7c-a673-17f6362dae64",
   "metadata": {},
   "source": [
    "### Conclusions from building the KG\n",
    "\n",
    "- we can visualize the graph in Neo4j Bloom directly on top of Neo4j local or Neo4j AuraDB\n",
    "- we have 1 Node type: Entity\n",
    "    ```cypher\n",
    "    MATCH (n)\n",
    "    RETURN distinct labels(n)[0] as label, count(n) as node_count\n",
    "    ```\n",
    "- 80 Relationship type\n",
    "    ```cypher\n",
    "    MATCH p=()-->() RETURN count(p)\n",
    "    ```\n",
    "- Entities only have 1 field called `id`, we don't have entity type like \"Person\" etc... NER would be needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bfdbb8-2dcc-4f1d-9e14-1d39c2d75b41",
   "metadata": {},
   "source": [
    "## 3. Create VectorStoreIndex for RAG\n",
    "\n",
    "References:\n",
    "- [Neo4jVectorDemo](https://docs.llamaindex.ai/en/latest/examples/vector_stores/Neo4jVectorDemo.html#)\n",
    "- [Neo4jVectorStore](https://docs.llamaindex.ai/en/stable/api/llama_index.vector_stores.Neo4jVectorStore.html#llama_index.vector_stores.Neo4jVectorStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e2b50f8-1542-469a-a0c7-6ed2d89c28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import Neo4jVectorStore\n",
    "\n",
    "neo4j_vector = Neo4jVectorStore(\n",
    "    username=username, \n",
    "    password=password, \n",
    "    url=url, \n",
    "    # \"local:BAAI/bge-small-en\" has dim of 384 https://huggingface.co/BAAI/bge-small-en\n",
    "    embedding_dimension=384,\n",
    "    database=database,\n",
    "    index_name='vector'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b8a50da-9afb-42e6-affd-1e8acd357b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(vector_store=neo4j_vector)\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context_vector, \n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b78d23-05aa-49f1-ac80-f1233a1b24d1",
   "metadata": {},
   "source": [
    "At this stage we can check with a cypher query that the stored embeddings have the right dimension 384\n",
    "```cypher\n",
    "MATCH (n:Chunk) RETURN size(n.embedding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef8f11-90bb-4fb2-80d0-7cbe8101ccd7",
   "metadata": {},
   "source": [
    "### Conclusions from building the Vector DB\n",
    "- there are 16 chunks for 1 wikipedia page, they contain the text, the embedding, and ids\n",
    "- there are no relationships\n",
    "- the chunks are not related to entities at all\n",
    "- the chunks get dumped into the same Graph database than the entity graph (unless you pass a different parameter), but for simplicity with Bloom viz, I chose to use the same database\n",
    "- the embedding is the embedding for the chunk which can be more or less long so more or less \"averaged\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704f1a8-19e6-4da5-8721-81ec48f43377",
   "metadata": {},
   "source": [
    "## 4. (Optional) Persist and Load from disk Llama Indexes\n",
    "### 4.1. Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f1d36-f3d0-4fbb-8a85-0430daf7b120",
   "metadata": {},
   "source": [
    "### 4.2. Restore\n",
    "In Llama Index, there are two scenarios we could apply Graph RAG:\n",
    "\n",
    "- Build Knowledge Graph from documents with Llama Index, with LLM or even local models, to do this, we should go for `KnowledgeGraphIndex`.\n",
    "\n",
    "- Leveraging existing Knowledge Graph, in this case, we should use `KnowledgeGraphRAGQueryEngine`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "617a99a9-c363-4a4d-8974-14d23da7c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_vector = Neo4jVectorStore(\n",
    "    username,\n",
    "    password,\n",
    "    url,\n",
    "    384,\n",
    "    index_name=\"vector\",\n",
    "    text_node_property=\"text\",\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    existing_vector,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a23fd7-8795-4bc0-8eb2-92ceddd94dd9",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a13261-1337-4b40-b120-a71bdc457142",
   "metadata": {},
   "source": [
    "### 5.1 Graph RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a337c6-aa13-4e4f-b19a-1a89560f0288",
   "metadata": {},
   "source": [
    "There are issues with the llama_index docs as I describe [here](https://github.com/run-llama/llama_index/issues/10474)\n",
    "\n",
    "reference:\n",
    "\n",
    "- [KGTableRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/kg.html#llama_index.indices.knowledge_graph.retrievers.KGTableRetriever) ??\n",
    "- [KnowledgeGraphRAGRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/kg.html#llama_index.indices.knowledge_graph.retrievers.KnowledgeGraphRAGRetriever) \"Retriever that perform SubGraph RAG towards knowledge graph.\"\n",
    "- [RetrieverQueryEngine](https://docs.llamaindex.ai/en/stable/api_reference/query/query_engines/retriever_query_engine.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d3bfebc-87f9-4d36-be7c-fbc0bf5fd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we just built the index and we have it available\n",
    "kg_rag_query_engine = index.as_query_engine(\n",
    "    include_text=False, \n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdfe46f6-19d6-489c-ae12-960151451cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.knowledge_graph.retrievers import KGTableRetriever\n",
    "\n",
    "assert type(kg_rag_query_engine.retriever) == KGTableRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c571e35-9542-4941-b75f-1f56e9b4aca4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# in case we want to restore it\n",
    "# from llama_index.query_engine import RetrieverQueryEngine\n",
    "# from llama_index.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "# # here I use a KnowledgeGraphRAGRetriever instead of a KGTableRetriever\n",
    "# graph_rag_retriever = KnowledgeGraphRAGRetriever(\n",
    "#     storage_context=storage_context,\n",
    "#     service_context=service_context,\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# kg_rag_query_engine = RetrieverQueryEngine.from_args(\n",
    "#     graph_rag_retriever,\n",
    "#     service_context=service_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2b148-5f7d-4230-8050-d1dbbf45e834",
   "metadata": {},
   "source": [
    "### 5.2. Vector RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9ef3eae-40b0-4864-a4b4-1226b096f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63fb40a7-573f-48a5-b727-bfea5584a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "\n",
    "assert type(vector_rag_query_engine) == RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205e6af-142f-4b69-9eae-b6fad671bb07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.3 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29bb6991-248e-4e8f-b600-20cf0bd1f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ade324fe-e7d8-43fa-905f-73957b878bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97a816cd-dd35-41e3-9913-374e2c277771",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cce79-aada-41ef-9713-fe6558e72ae1",
   "metadata": {},
   "source": [
    "## 6. Query with the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee7aceb-bd1f-4aed-8116-1a18b2fbf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d147e85f-961e-4276-9a5b-305d353acb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill is the leader of the Guardians of the Galaxy. He is known to have a half-sister named Mantis, who is also a member of the Guardians. The Guardians have traveled to various locations including Orgocorp's headquarters and Counter-earth, and have been attacked by Adam warlock. Quill has repaid a favor to James Gunn and has stated intentions for future projects in the Marvel Cosmic Universe. He has also been known to use offensive language and has expressed reluctance to return to the franchise without certain characters. Quill was confirmed to return to write and direct Guardians of the Galaxy vol. 3 in April 2017.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d7c9673-c888-4f70-93d1-d82a9d8755cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.IllegalArgumentException: Index query vector has 384 dimensions, but indexed vectors have 1536.}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response_vector_rag \u001b[38;5;241m=\u001b[39m \u001b[43mvector_rag_query_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about Peter Quill.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<b>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_vector_rag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</b>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/core/base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     39\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/query_engine/retriever_query_engine.py:171\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    169\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    170\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 171\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[1;32m    173\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[1;32m    174\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    177\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/query_engine/retriever_query_engine.py:127\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m--> 127\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[38;5;241m=\u001b[39mquery_bundle)\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/core/base_retriever.py:224\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    221\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[1;32m    222\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    223\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[0;32m--> 224\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[1;32m    226\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    227\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[1;32m    228\u001b[0m         )\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:92\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     87\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39membed_model\u001b[38;5;241m.\u001b[39mget_agg_embedding_from_queries(\n\u001b[1;32m     89\u001b[0m                 query_bundle\u001b[38;5;241m.\u001b[39membedding_strs\n\u001b[1;32m     90\u001b[0m             )\n\u001b[1;32m     91\u001b[0m         )\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:168\u001b[0m, in \u001b[0;36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[0;34m(self, query_bundle_with_embeddings)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_nodes_with_embeddings\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[NodeWithScore]:\n\u001b[1;32m    167\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_vector_store_query(query_bundle_with_embeddings)\n\u001b[0;32m--> 168\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_node_list_from_query_result(query_result)\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/vector_stores/neo4jvector.py:375\u001b[0m, in \u001b[0;36mNeo4jVectorStore.query\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m read_query \u001b[38;5;241m=\u001b[39m _get_search_index_query(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhybrid_search) \u001b[38;5;241m+\u001b[39m retrieval_query\n\u001b[1;32m    367\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_name,\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: query\u001b[38;5;241m.\u001b[39msimilarity_top_k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: remove_lucene_chars(query\u001b[38;5;241m.\u001b[39mquery_str),\n\u001b[1;32m    373\u001b[0m }\n\u001b[0;32m--> 375\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m nodes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    378\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/vector_stores/neo4jvector.py:331\u001b[0m, in \u001b[0;36mNeo4jVectorStore.database_query\u001b[0;34m(self, query, params)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     data \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(query, params)\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CypherSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCypher Statement is not valid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/vector_stores/neo4jvector.py:331\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     data \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(query, params)\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [r\u001b[38;5;241m.\u001b[39mdata() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CypherSyntaxError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCypher Statement is not valid\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/neo4j/_sync/work/result.py:270\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/neo4j/_sync/io/_bolt.py:849\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m    846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    847\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[1;32m    848\u001b[0m )\n\u001b[0;32m--> 849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/neo4j/_sync/io/_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[0;34m(self, tag, fields)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/neo4j/_sync/io/_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[0;34m(self, metadata)\u001b[0m\n\u001b[1;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[0;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `db.index.vector.queryNodes`: Caused by: java.lang.IllegalArgumentException: Index query vector has 384 dimensions, but indexed vectors have 1536.}"
     ]
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e7e98be-4d5f-4177-86c4-fbb33f98483a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_vector_rag_query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response_graph_vector_rag \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_vector_rag_query_engine\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about Peter Quill.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<b>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_graph_vector_rag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</b>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_vector_rag_query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006632bc-b6a7-422e-8b73-17ebbef9b8e1",
   "metadata": {},
   "source": [
    "## 7. Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d7bec1f-224f-44d3-808a-e0bef3879e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = llm.complete(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e90e7cc-2d1a-4e6b-a7d7-0e96282ae280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " | Fact | Graph Result | Vector Result | Both Results |\n",
       "| --- | --- | --- | --- |\n",
       "| Character Name | Peter Quill / Star-Lord | Peter Quill / Star-Lord | Peter Quill / Star-Lord |\n",
       "| Universe | Marvel Comics | Marvel Cinematic Universe | Marvel Universe |\n",
       "| Creation | Created by Dan Abnett and Andy Lanning, first appeared in \"Annihilators\" #1 in July 2006 | Portrayed by Chris Pratt in films since 2014 | Created, first appearance in respective universes |\n",
       "| Species | Human from Earth | Human who was abducted from Earth as a child and grew up among extraterrestrial beings | Human, abducted from Earth as a child, raised among aliens |\n",
       "| Birth Year | Born in 1982 | N/A | Born in 1982 |\n",
       "| Backstory | Raised among the Ravagers, skilled mercenary and thief, leads Guardians of the Galaxy | Skilled in combat and piloting spaceships, quirky sense of humor, love for classic Earth music, depicted as more traditional superhero compared to the Guardians | Raised among the Ravagers, became a space adventurer, joined the Guardians of the Galaxy |\n",
       "| Weaponry | Star-Lord gun, Orb, Milano spaceship | N/A | Advanced weaponry, including Star-Lord gun and Orb, Milano spaceship |\n",
       "| Personality | Charming, rebellious, sarcastic, strong sense of loyalty to those he cares about, romantically involved with Gamora and Rocket Raccoon | Quirky sense of humor, love for classic Earth music, depicted as a more traditional superhero compared to the Guardians | Charming, rebellious, sarcastic, romantic relationships with Gamora and Rocket Raccoon |\n",
       "| Films | N/A | Appeared in \"Guardians of the Galaxy\" (2014), \"Guardians of the Galaxy Vol. 2\" (2017), \"Avengers: Infinity War\" (2018), and \"Avengers: Endgame\" (2018) | N/A, appears in films |\n",
       "| Director | N/A | James Gunn was fired from the franchise over old tweets, but negotiations to return ultimately failed | N/A, no mention of director |\n",
       "| Depiction in \"Vol. 3\" | In a state of depression following Gamora's appearance | N/A | In a state of depression following Gamora's appearance in \"Guardians of the Galaxy Vol. 3\" |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(analysis.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b3b68-dee8-4535-a1ef-8d28d67e3e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
