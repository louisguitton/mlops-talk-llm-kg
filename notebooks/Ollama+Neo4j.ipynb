{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e6ca89-674d-4327-90d4-db1a5920a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f04381e-896a-4ee4-913d-07b23e5ee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac12660-412e-44d8-aafe-b83a1f0f5eab",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "### 1.1 Prepare LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60720ddb-2a55-4b97-a0de-e15be24a57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "OLLAMA_HOST = 'localhost'\n",
    "OLLAMA_MODEL = 'mistral'\n",
    "llm = Ollama(model=OLLAMA_MODEL, base_url=\"http://\"+OLLAMA_HOST+\":11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bb1371-01bf-40cb-b56a-58bc219ba532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis.guitton/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    # To save costs, we use a local model.\n",
    "    # This will use a well-performing and fast default from Hugging Face.\n",
    "    # this model has dim of 384 https://huggingface.co/BAAI/bge-small-en\n",
    "    embed_model=\"local:BAAI/bge-small-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285d07a-8d12-4dee-b3cf-4fd1e1a863b5",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Graph Store\n",
    "`Neo4j` is supported as a graph store integration. You can persist, visualize, and query graphs using LlamaIndex and Neo4j. Furthermore, existing Neo4j graphs are directly supported using `text2cypher` and the `KnowledgeGraphQueryEngine`.\n",
    "\n",
    "If you’ve never used Neo4j before, you can download the desktop client [here](https://neo4j.com/download/).\n",
    "\n",
    "Once you open the client, create a new project and install the `apoc` integration. Full instructions here. Just click on your project, select `Plugins` on the left side menu, install APOC and restart your server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d15c05-009d-4677-b1a9-1da5ed67651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"neo4j\"\n",
    "password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "# Neo4j cloud has a generous free tier, so I use that instead of localhost\n",
    "url = \"bolt://localhost:7687\"\n",
    "# url = \"bolt+s://3b2530f1.databases.neo4j.io:7687\"\n",
    "database = \"neo4j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a2128b-9575-4a2b-bc0d-bfcdcde5a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores import Neo4jGraphStore\n",
    "\n",
    "graph_store = Neo4jGraphStore(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    url=url,\n",
    "    database=database,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751a5075-72d7-4fbd-99af-3ca75de8dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50bd077-5864-4fb7-8a25-793e9eaa69fd",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "### 2.1 Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a8e7d3-349b-4f37-8bcb-2ecebb819833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fcf49-16f5-47b5-a2a3-561fd25211ff",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to Graph\n",
    "reference:\n",
    "- [KnowledgeGraphIndex](https://docs.llamaindex.ai/en/stable/api_reference/indices/kg.html#llama_index.indices.knowledge_graph.KnowledgeGraphIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd75ae9-d71a-4a6e-be19-ba38cbf9a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Parsing nodes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.50it/s]\n",
      "Processing nodes:   0%|                                                                                                                                               | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings:  42%|█████████████████████████████████████████████████████▊                                                                           | 10/24 [00:01<00:01,  9.03it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 18.87it/s]\u001b[A\n",
      "Processing nodes:   6%|████████▍                                                                                                                              | 1/16 [00:22<05:35, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 30.72it/s]\u001b[A\n",
      "Processing nodes:  12%|████████████████▉                                                                                                                      | 2/16 [00:35<03:58, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 78.36it/s]\u001b[A\n",
      "Processing nodes:  19%|█████████████████████████▎                                                                                                             | 3/16 [00:47<03:10, 14.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.43it/s]\u001b[A\n",
      "Processing nodes:  25%|█████████████████████████████████▊                                                                                                     | 4/16 [00:59<02:42, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 58.23it/s]\u001b[A\n",
      "Processing nodes:  31%|██████████████████████████████████████████▏                                                                                            | 5/16 [01:12<02:25, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 55.04it/s]\u001b[A\n",
      "Processing nodes:  38%|██████████████████████████████████████████████████▋                                                                                    | 6/16 [01:24<02:08, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 27.58it/s]\u001b[A\n",
      "Processing nodes:  44%|███████████████████████████████████████████████████████████                                                                            | 7/16 [01:37<01:58, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 14.40it/s]\u001b[A\n",
      "Processing nodes:  50%|███████████████████████████████████████████████████████████████████▌                                                                   | 8/16 [01:45<01:31, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.83it/s]\u001b[A\n",
      "Processing nodes:  56%|███████████████████████████████████████████████████████████████████████████▉                                                           | 9/16 [02:00<01:26, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 28.12it/s]\u001b[A\n",
      "Processing nodes:  62%|███████████████████████████████████████████████████████████████████████████████████▊                                                  | 10/16 [02:17<01:23, 13.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.39it/s]\u001b[A\n",
      "Processing nodes:  69%|████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 11/16 [02:30<01:08, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 60.27it/s]\u001b[A\n",
      "Processing nodes:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 12/16 [02:45<00:56, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 23.94it/s]\u001b[A\n",
      "Processing nodes:  81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 13/16 [03:02<00:44, 14.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 25.50it/s]\u001b[A\n",
      "Processing nodes:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 14/16 [03:17<00:29, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 39.40it/s]\u001b[A\n",
      "Processing nodes:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 15/16 [03:31<00:14, 14.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.38it/s]\u001b[A\n",
      "Processing nodes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:42<00:00, 13.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True,\n",
    "    show_progress=True\n",
    "    # max_object_length: int = 128,\n",
    "    \n",
    "    # to extract triplets, kg_triplet_extract_fn is used if not None,\n",
    "    # kg_triplet_extract_fn: Optional[Callable] = None, \n",
    "    # else, the LLM from the service context is used with the kg_triple_extract_template if not None else the default triplet extract prompt\n",
    "    # kg_triple_extract_template: Optional[BasePromptTemplate] = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44c81d-8dd9-4a7c-a673-17f6362dae64",
   "metadata": {},
   "source": [
    "### Conclusions from building the KG\n",
    "\n",
    "- we can visualize the graph in Neo4j Bloom directly on top of Neo4j local or Neo4j AuraDB\n",
    "- we have 1 Node type: Entity\n",
    "    ```cypher\n",
    "    MATCH (n)\n",
    "    RETURN distinct labels(n)[0] as label, count(n) as node_count\n",
    "    ```\n",
    "- 80 Relationship type\n",
    "    ```cypher\n",
    "    MATCH p=()-->() RETURN count(p)\n",
    "    ```\n",
    "- Entities only have 1 field called `id`, we don't have entity type like \"Person\" etc... NER would be needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bfdbb8-2dcc-4f1d-9e14-1d39c2d75b41",
   "metadata": {},
   "source": [
    "## 3. Create VectorStoreIndex for RAG\n",
    "\n",
    "References:\n",
    "- [Neo4jVectorDemo](https://docs.llamaindex.ai/en/latest/examples/vector_stores/Neo4jVectorDemo.html#)\n",
    "- [Neo4jVectorStore](https://docs.llamaindex.ai/en/stable/api/llama_index.vector_stores.Neo4jVectorStore.html#llama_index.vector_stores.Neo4jVectorStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e2b50f8-1542-469a-a0c7-6ed2d89c28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import Neo4jVectorStore\n",
    "\n",
    "# \"local:BAAI/bge-small-en\" has dim of 384 https://huggingface.co/BAAI/bge-small-en\n",
    "embed_dim = len(service_context.embed_model.get_text_embedding(\"I want to check embed dimensions\"))\n",
    "\n",
    "neo4j_vector = Neo4jVectorStore(\n",
    "    username=username, \n",
    "    password=password, \n",
    "    url=url, \n",
    "    embedding_dimension=embed_dim,\n",
    "    database=database,\n",
    "    index_name='vector'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8a50da-9afb-42e6-affd-1e8acd357b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(vector_store=neo4j_vector)\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context_vector, \n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b78d23-05aa-49f1-ac80-f1233a1b24d1",
   "metadata": {},
   "source": [
    "At this stage we can check with a cypher query that the stored embeddings have the right dimension 384\n",
    "```cypher\n",
    "MATCH (n:Chunk) RETURN size(n.embedding)\n",
    "```\n",
    "And make sure your Neo4j index \"vector\" has the right dimension in its config\n",
    "```cypher\n",
    "SHOW INDEXES\n",
    "YIELD name, type, indexProvider AS provider, options, createStatement\n",
    "RETURN name, type, provider, options.indexConfig AS config, createStatement\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef8f11-90bb-4fb2-80d0-7cbe8101ccd7",
   "metadata": {},
   "source": [
    "### Conclusions from building the Vector DB\n",
    "- there are 16 chunks for 1 wikipedia page, they contain the text, the embedding, and ids\n",
    "- there are no relationships\n",
    "- the chunks are not related to entities at all\n",
    "- the chunks get dumped into the same Graph database than the entity graph (unless you pass a different parameter), but for simplicity with Bloom viz, I chose to use the same database\n",
    "- the embedding is the embedding for the chunk which can be more or less long so more or less \"averaged\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704f1a8-19e6-4da5-8721-81ec48f43377",
   "metadata": {},
   "source": [
    "## 4. (Optional) Persist and Load from disk Llama Indexes\n",
    "### 4.1. Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f1d36-f3d0-4fbb-8a85-0430daf7b120",
   "metadata": {},
   "source": [
    "### 4.2. Restore\n",
    "In Llama Index, there are two scenarios we could apply Graph RAG:\n",
    "\n",
    "- Build Knowledge Graph from documents with Llama Index, with LLM or even local models, to do this, we should go for `KnowledgeGraphIndex`.\n",
    "\n",
    "- Leveraging existing Knowledge Graph, in this case, we should use `KnowledgeGraphRAGQueryEngine`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "617a99a9-c363-4a4d-8974-14d23da7c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_vector = Neo4jVectorStore(\n",
    "    username,\n",
    "    password,\n",
    "    url,\n",
    "    384,\n",
    "    index_name=\"vector\",\n",
    "    text_node_property=\"text\",\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    existing_vector,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a23fd7-8795-4bc0-8eb2-92ceddd94dd9",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a13261-1337-4b40-b120-a71bdc457142",
   "metadata": {},
   "source": [
    "### 5.1 Graph RAG query engine\n",
    "#### 5.1.1 Choose a retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a337c6-aa13-4e4f-b19a-1a89560f0288",
   "metadata": {},
   "source": [
    "There are issues with the llama_index docs as I describe [here](https://github.com/run-llama/llama_index/issues/10474)\n",
    "\n",
    "reference:\n",
    "\n",
    "- [KGTableRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/kg.html#llama_index.indices.knowledge_graph.retrievers.KGTableRetriever) ??\n",
    "- [KnowledgeGraphRAGRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/kg.html#llama_index.indices.knowledge_graph.retrievers.KnowledgeGraphRAGRetriever) \"Retriever that perform SubGraph RAG towards knowledge graph.\"\n",
    "- [RetrieverQueryEngine](https://docs.llamaindex.ai/en/stable/api_reference/query/query_engines/retriever_query_engine.html)\n",
    "\n",
    "See [Retriever Modes](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/retriever_modes.html) for a full list of (index-specific) retriever modes and the retriever classes they map to.\n",
    "\n",
    "For `KnowledgeGraphIndex`, we have a choice of `keyword`, `embedding` or `hybrid`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e1f9d0e-63ca-42b5-b81e-ec4be7e85ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 5401f462-d91a-4e71-98b7-6f43efb5b6d0: === Filming ===\n",
      "Principal photography began on November 8, 2021, at Trilith S...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "=== Filming ===\n",
       "Principal photography began on November 8, 2021, at Trilith Studios in Atlanta, Georgia, under the working title Hot Christmas. Henry Braham serves as cinematographer, after doing so for Vol. 2, The Suicide Squad,  and The Guardians of the Galaxy Holiday Special. Filming was previously scheduled to begin in January or February 2019 prior to Gunn's firing, and then in February 2021, before Gunn began work on Peacemaker. With the start of filming, Sylvester Stallone revealed that he would return as Stakar Ogord from Vol. 2, and Gunn posted a photo of the main cast members which revealed that Chukwudi Iwuji was part of the film following his collaboration with Gunn on Peacemaker. Iwuji's screen test for the film was shot on the set of Peacemaker with that series' crew, and Marvel repaid this favor by letting Gunn use the Vol. 3 set and crew to film Ezra Miller's cameo appearance as Barry Allen / The Flash for the Peacemaker season finale.Production designer Beth Mickle said Gunn chose to mainly use practical effects for Vol. 3 after they did so with their work on The Suicide Squad. In February 2021, Gunn stated the film would be shot using Industrial Light & Magic's StageCraft virtual production technology that was developed for the Disney+ Star Wars series The Mandalorian, but in October, he said they would not be able to use the technology because the sets were too big, believing they were larger than the sets used on The Suicide Squad. The interior of the Guardians' new ship, the Bowie, was a four-story set. Judianna Makovsky serves as costume designer. The Guardians of the Galaxy Holiday Special was filmed at the same time as Vol. 3, from February to late April 2022, with the same main cast and sets. Gunn enjoyed being able to switch to filming the special after doing scenes for Vol. 3, given the tonal difference between the two with Vol. 3 being more \"emotional\", feeling it helped provide a \"relief\" to the actors as well, and called the Holiday Special shoot easier than Vol. 3. In February 2022, Callie Brand was revealed to appear in the film as an alien. Shooting was also expected to occur in London, England, in late 2021. Vol. 3 is the first MCU film to feature the word \"fuck\" uncensored; it is spoken by Quill. The dialogue \"Open the fucking door\" was not scripted, but improvised by Pratt on Gunn's suggestion, though there was some consideration over giving the one-liner to Groot instead. Gunn mentioned that the Korean film The Villainess (2017) inspired one of the fight scenes in Vol. 3. Filming on Vol. 3 wrapped on May 6, 2022."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The following are knowledge sequence in max depth 2 in the form of directed graph like:\n",
       "`subject -[predicate]->, object, <-[predicate_next_hop]-, object_next_hop ...`\n",
       "['SPEAKS_UNCENSORED_\"FUCK\"_IN', 'Vol. 3']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = kg_index.as_retriever(\n",
    "    retriever_mode=\"keyword\" # =\"embedding\" # =\"hybrid\"\n",
    ")\n",
    "nodes = retriever.retrieve(\"Tell me about Peter Quill.\")\n",
    "for node in nodes:\n",
    "    display(Markdown(node.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e22c91eb-166c-49c1-a915-ee72f747bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 5401f462-d91a-4e71-98b7-6f43efb5b6d0: === Filming ===\n",
      "Principal photography began on November 8, 2021, at Trilith S...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 32627b8e-e016-474e-b5fe-808e6f5c47e5: Guardians of the Galaxy Vol. 3 (stylized in marketing as Guardians of the Gal...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 9522464d-0f5e-423a-a26f-b378af1c496a: == Cast ==\n",
      "Chris Pratt as Peter Quill / Star-Lord:The half-human, half-Celest...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "=== Filming ===\n",
       "Principal photography began on November 8, 2021, at Trilith Studios in Atlanta, Georgia, under the working title Hot Christmas. Henry Braham serves as cinematographer, after doing so for Vol. 2, The Suicide Squad,  and The Guardians of the Galaxy Holiday Special. Filming was previously scheduled to begin in January or February 2019 prior to Gunn's firing, and then in February 2021, before Gunn began work on Peacemaker. With the start of filming, Sylvester Stallone revealed that he would return as Stakar Ogord from Vol. 2, and Gunn posted a photo of the main cast members which revealed that Chukwudi Iwuji was part of the film following his collaboration with Gunn on Peacemaker. Iwuji's screen test for the film was shot on the set of Peacemaker with that series' crew, and Marvel repaid this favor by letting Gunn use the Vol. 3 set and crew to film Ezra Miller's cameo appearance as Barry Allen / The Flash for the Peacemaker season finale.Production designer Beth Mickle said Gunn chose to mainly use practical effects for Vol. 3 after they did so with their work on The Suicide Squad. In February 2021, Gunn stated the film would be shot using Industrial Light & Magic's StageCraft virtual production technology that was developed for the Disney+ Star Wars series The Mandalorian, but in October, he said they would not be able to use the technology because the sets were too big, believing they were larger than the sets used on The Suicide Squad. The interior of the Guardians' new ship, the Bowie, was a four-story set. Judianna Makovsky serves as costume designer. The Guardians of the Galaxy Holiday Special was filmed at the same time as Vol. 3, from February to late April 2022, with the same main cast and sets. Gunn enjoyed being able to switch to filming the special after doing scenes for Vol. 3, given the tonal difference between the two with Vol. 3 being more \"emotional\", feeling it helped provide a \"relief\" to the actors as well, and called the Holiday Special shoot easier than Vol. 3. In February 2022, Callie Brand was revealed to appear in the film as an alien. Shooting was also expected to occur in London, England, in late 2021. Vol. 3 is the first MCU film to feature the word \"fuck\" uncensored; it is spoken by Quill. The dialogue \"Open the fucking door\" was not scripted, but improvised by Pratt on Gunn's suggestion, though there was some consideration over giving the one-liner to Groot instead. Gunn mentioned that the Korean film The Villainess (2017) inspired one of the fight scenes in Vol. 3. Filming on Vol. 3 wrapped on May 6, 2022."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Guardians of the Galaxy Vol. 3 (stylized in marketing as Guardians of the Galaxy Volume 3) is a 2023 American superhero film based on the Marvel Comics superhero team Guardians of the Galaxy, produced by Marvel Studios, and distributed by Walt Disney Studios Motion Pictures. It is the sequel to Guardians of the Galaxy (2014) and Guardians of the Galaxy Vol. 2 (2017), and the 32nd film in the Marvel Cinematic Universe (MCU). Written and directed by James Gunn, it features an ensemble cast including Chris Pratt, Zoe Saldaña, Dave Bautista, Karen Gillan, Pom Klementieff, Vin Diesel, Bradley Cooper, Will Poulter, Sean Gunn, Chukwudi Iwuji, Linda Cardellini, Nathan Fillion, and Sylvester Stallone. In the film, the Guardians must save Rocket's (Cooper) life from the High Evolutionary (Iwuji).\n",
       "Gunn stated in November 2014 that he had initial ideas for a third and final film in the series, and announced his return to write and direct in April 2017. Disney fired him from the film in July 2018 following the resurfacing of controversial posts on Twitter, but the studio reversed course by that October and reinstated him. Gunn's return was publicly revealed in March 2019, with production resuming after he completed work for DC on The Suicide Squad (2021) and the first season of its spin-off series Peacemaker (2022). Filming began in November 2021 at Trilith Studios in Atlanta, Georgia, and lasted until May 2022.\n",
       "Guardians of the Galaxy Vol. 3 premiered at Disneyland Paris on April 22, 2023, and was released in the United States on May 5, as part of Phase Five of the MCU. Like its predecessors, it was a critical and commercial success, with many deeming it to be a satisfactory conclusion to the trilogy. It grossed over $848.6 million worldwide, becoming the fourth-highest-grossing film of 2023. At the 96th Academy Awards, the film was nominated for Best Visual Effects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "== Cast ==\n",
       "Chris Pratt as Peter Quill / Star-Lord:The half-human, half-Celestial leader of the Guardians of the Galaxy who was abducted from Earth as a child and raised by a group of alien thieves and smugglers, the Ravagers. In the film, Quill is in a \"state of depression\" following the appearance of a variant of his dead lover Gamora, who does not share the same affection for Quill as her older version had for him, which in turn affects his leadership of the Guardians.\n",
       "Zoe Saldaña as Gamora:An orphan who seeks redemption for her past crimes, and was adopted and trained by Thanos to be his personal assassin. The original version of Gamora, a member of the Guardians, was killed by Thanos in Avengers: Infinity War (2018), and an alternate version of the character traveled to the present in Avengers: Endgame (2019); Saldaña reprises the latter role in this film, now serving as a member of the Ravagers. Saldaña stated that Vol. 3 would be the final time she would portray Gamora, noting that she originally signed to play her in one film and ended up playing the role for much longer, a role she was grateful to play due to the impact it especially had on female fans.\n",
       "Dave Bautista as Drax the Destroyer:A member of the Guardians and a highly skilled warrior whose family was slaughtered by Ronan the Accuser, under the instructions of Thanos. Bautista stated that Vol. 3 would be the final time he would portray Drax, having been grateful for the role, while still calling it a \"relief\" to have concluded his time with the character, given the long hours needed to get into makeup and hoping to pursue more dramatic acting roles. Because of Bautista's decision, Gunn opted not to include Drax in the post-credits scene.\n",
       "Karen Gillan as Nebula:A member of the Guardians, a former Avenger, and Gamora's adoptive sister who, similarly to her, was trained by their adoptive father Thanos to be his personal assassin. Gillan believed Nebula was developing into a \"slightly different person\" with more levity as she starts to heal psychologically following the death of Thanos, who was the source of her abuse and torment. Vol. 3 fulfills a character arc for the character writer and director James Gunn envisioned when starting work on Guardians of the Galaxy (2014), going from a minor villain to a member of the Guardians. Although the film teases a possible romance between Star-Lord and Nebula, Gunn denies having ever considered the two becoming a couple, though Gillan does believe she harbors a small crush on Quill.\n",
       "Pom Klementieff as Mantis: A member of the Guardians with empathic powers, and Quill's half-sister.\n",
       "Vin Diesel as Groot: A member of the Guardians who is a tree-like humanoid and the accomplice of Rocket. Austin Freeman provided the motion-capture for Groot.\n",
       "Bradley Cooper as Rocket:A member of the Guardians and a former Avenger who is a genetically engineered raccoon-based bounty hunter and a master of weapons and military tactics. Gunn said that the film tells Rocket's story, including his background and \"where he's going\", along with how that ties into the other Guardians and the end of this iteration of the team. The film completes a character arc that was established in Guardians of the Galaxy and Guardians of the Galaxy Vol. 2 (2017), and continued in Infinity War and Endgame. Sean Gunn once again provided on set motion capture for the character, while also voicing young Rocket. Cooper also voiced adolescent Rocket while Noah Raskin voiced baby Rocket.\n",
       "Will Poulter as Adam Warlock:A powerful artificial being created by the Sovereign to destroy the Guardians. Given Warlock is newly born from the Sovereign's cocoon, he is \"basically a baby\" that \"does not understand life very well\". Poulter believed there was \"a lot of comedy\" in someone just entering the world for the first time and \"trying to develop his moral compass\", while also having \"some genuine pathos\". Gunn thought Warlock's interactions with the Guardians provided \"an interesting juxtaposition\" to what their journey has been, and described him as a more traditional superhero compared to the Guardians, although not necessarily a hero.\n",
       "Sean Gunn as Kraglin: A member of the Guardians and Yondu Udonta's former second-in-command in the Ravagers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The following are knowledge sequence in max depth 2 in the form of directed graph like:\n",
       "`subject -[predicate]->, object, <-[predicate_next_hop]-, object_next_hop ...`\n",
       "('Chris pratt', 'Plays character of', 'Peter quill / star-lord')\n",
       "('Quill', 'Speaks uncensored \"fuck\" in', 'Vol. 3')\n",
       "['SPEAKS_UNCENSORED_\"FUCK\"_IN', 'Vol. 3']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    # graph_query_synthesis_prompt=\n",
    "    # graph_response_answer_prompt=\n",
    ")\n",
    "nodes = retriever.retrieve(\"Tell me about Peter Quill.\")\n",
    "for node in nodes:\n",
    "    display(Markdown(node.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff8802-c662-4570-b41c-4f2c78b33edd",
   "metadata": {},
   "source": [
    "#### 5.1.2 Build a query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d3bfebc-87f9-4d36-be7c-fbc0bf5fd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we just built the index and we have it available\n",
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False, \n",
    "    retriever_mode=\"hybrid\",\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "# from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# kg_rag_query_engine = RetrieverQueryEngine.from_args(\n",
    "#     graph_rag_retriever,\n",
    "#     service_context=service_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdfe46f6-19d6-489c-ae12-960151451cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.knowledge_graph.retrievers import KGTableRetriever\n",
    "\n",
    "assert type(kg_rag_query_engine.retriever) == KGTableRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2b148-5f7d-4230-8050-d1dbbf45e834",
   "metadata": {},
   "source": [
    "### 5.2. Vector RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9ef3eae-40b0-4864-a4b4-1226b096f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63fb40a7-573f-48a5-b727-bfea5584a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "\n",
    "assert type(vector_rag_query_engine) == RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205e6af-142f-4b69-9eae-b6fad671bb07",
   "metadata": {},
   "source": [
    "### 5.3 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29bb6991-248e-4e8f-b600-20cf0bd1f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ade324fe-e7d8-43fa-905f-73957b878bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97a816cd-dd35-41e3-9913-374e2c277771",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cce79-aada-41ef-9713-fe6558e72ae1",
   "metadata": {},
   "source": [
    "## 6. Query with the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ee7aceb-bd1f-4aed-8116-1a18b2fbf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d147e85f-961e-4276-9a5b-305d353acb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill is a subject in the provided knowledge sequence, specifically connected to the object \"Vol. 3\" through the relationship \"SPEAKS\\_UNCENSORED\\_'FUCK'\\_IN\". However, without further context or information, it is unclear what specific properties or characteristics belong to Peter Quill based on the given schema. Therefore, I cannot provide a comprehensive answer about Peter Quill beyond this connection.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d7c9673-c888-4f70-93d1-d82a9d8755cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill is a character in the Marvel Cinematic Universe, portrayed by Chris Pratt. He is the half-human, half-Celestial leader of the Guardians of the Galaxy. Quill was abducted from Earth as a child and raised by a group of alien thieves and smugglers, the Ravagers. In the films, Quill experiences depression following the appearance of an alternate version of his deceased lover Gamora, who does not share the same affections towards him. This affects his leadership of the Guardians.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e7e98be-4d5f-4177-86c4-fbb33f98483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill, also known as Star-Lord, is a character in the Marvel Cinematic Universe who serves as the leader of the Guardians of the Galaxy. He was abducted from Earth as a child and raised by a group of alien thieves and smugglers, the Ravagers. In the films, Quill is depicted as being in a state of depression following the appearance of an alternate version of his dead lover Gamora, who does not reciprocate his affections. He was portrayed by Chris Pratt in the first two Guardians of the Galaxy films and Avengers: Endgame. Pratt stated that he would be wrapping up his time with the character due to his contract ending and wanting to explore other roles.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006632bc-b6a7-422e-8b73-17ebbef9b8e1",
   "metadata": {},
   "source": [
    "## 7. Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d7bec1f-224f-44d3-808a-e0bef3879e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "analysis = llm.complete(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e90e7cc-2d1a-4e6b-a7d7-0e96282ae280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " | Fact | Result from Graph | Result from Vector | Result Graph+Vector |\n",
       "|-------|--------------------|---------------------|----------------------|\n",
       "| Name  | Peter Quill         | Peter Quill          | Peter Quill, Star-Lord |\n",
       "| Role  | Leader of the Guardians of the Galaxy | Character in Marvel Cinematic Universe | Leader of the Guardians of the Galaxy |\n",
       "| Connection to Object | Speaks uncensored 'FUCK' in Vol. 3 | Raised by Ravagers, abducted as a child | Abducted from Earth as a child, raised by the Ravagers |\n",
       "| Affiliation | - | Leader of Guardians of the Galaxy, Celestial-human hybrid | Leader of Guardians of the Galaxy |\n",
       "| Depiction in films | Unclear | Portrayed by Chris Pratt, experiences depression over Gamora | Portrayed by Chris Pratt, experiences depression over alternate Gamora |\n",
       "| Background | Half-human, half-Celestial, connection to Vol. 3 through 'Speaks\\_UNCENSORED\\_'FUCK'\\_IN' | Originated on Earth, abducted as a child, raised by Ravagers | Originated on Earth, abducted as a child, raised by Ravagers |\n",
       "| Emotional State | - | Experiences depression following appearance of alternate Gamora | Experiences depression following appearance of alternate Gamora |\n",
       "| Contractual Status | - | - | Pratt's contract ending, wanting to explore other roles |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(analysis.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b3b68-dee8-4535-a1ef-8d28d67e3e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
