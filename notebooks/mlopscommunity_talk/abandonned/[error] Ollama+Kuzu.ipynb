{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e57a93a-f437-47b7-a0c7-b81b937ce73b",
   "metadata": {},
   "source": [
    "References\n",
    "- https://docs.llamaindex.ai/en/latest/examples/query_engine/knowledge_graph_query_engine.html#\n",
    "- https://docs.llamaindex.ai/en/latest/examples/index_structs/knowledge_graph/KuzuGraphDemo.html#\n",
    "- https://github.com/sroecker/LLM_AppDev-HandsOn\n",
    "- [KG RAG query engine](https://colab.research.google.com/drive/1QpfgBvZZCg00WKNUgl6vZ1mgo-OfdynN#scrollTo=htIlL1nSiqFo)\n",
    "- [Rebel + LlamaIndex Knowledge Graph Query Engine](https://colab.research.google.com/drive/1KPZ1GpqIwU4sEeYSRFEI_74rbuUZwX_B#scrollTo=5mHzFSTbPbWf)\n",
    "- [Basic Knowledge Graph Index](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphDemo.html)\n",
    "- [Knowledge Graph Index + Vector Index](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html)\n",
    "- [Knowledge Graph Index + Nebula](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/NebulaGraphKGIndexDemo.html)\n",
    "- [Querying existing Nebula KGs](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_query_engine.html)\n",
    "- [Knowledge Graph Index + Kuzu](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KuzuGraphDemo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8523fea-6112-4821-ba2a-2c3fb362c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e375249-a467-43c2-88c9-60b9239078ed",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "### 1.1 Prepare LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015c22d9-ecf7-46cc-851b-48703fca475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "OLLAMA_HOST = 'localhost'\n",
    "OLLAMA_MODEL = 'mistral'\n",
    "llm = Ollama(model=OLLAMA_MODEL, base_url=\"http://\"+OLLAMA_HOST+\":11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2723852-7132-4ff4-8779-a7aa503b1c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis.guitton/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    embed_model=\"local\",\n",
    "    # chunk_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331a29b-9b89-4685-8035-23a60c22adfa",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Graph Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b5ebde-c0fb-4fe0-bb44-c8d4689732a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores import KuzuGraphStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36d0e288-0c1c-4c33-80de-dcf18c148ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(\"../data/kuzu-db-1\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83145d7c-18e7-4706-ad8d-239abda5bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kuzu\n",
    "\n",
    "db = kuzu.Database(\"../data/kuzu-db-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2af783-8deb-4ff2-ad7a-e1f2a2894fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = KuzuGraphStore(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39da28c-6e45-45c4-93bd-408b7de271ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f89f84-904f-457a-9d93-1264f4e4b5d9",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "### 2.1 Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c4ed93-3393-4429-94ae-1fa5f49a7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e3c98-417d-452f-9861-6dc6731657ab",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0389163-e68c-4afa-87f6-21c016eb0188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.94it/s]\n",
      "Processing nodes:   0%|                                                                                                                                               | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes:   0%|                                                                                                                                               | 0/16 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parameters must be a dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KnowledgeGraphIndex\n\u001b[0;32m----> 3\u001b[0m kg_index \u001b[38;5;241m=\u001b[39m \u001b[43mKnowledgeGraphIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_triplets_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# kg_triplet_extract_fn=extract_triplets\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/base.py:110\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    103\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    104\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     service_context\u001b[38;5;241m.\u001b[39mtransformations,\n\u001b[1;32m    106\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/knowledge_graph/base.py:82\u001b[0m, in \u001b[0;36mKnowledgeGraphIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, service_context, storage_context, kg_triple_extract_template, max_triplets_per_chunk, include_embeddings, show_progress, max_object_length, kg_triplet_extract_fn, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_object_length \u001b[38;5;241m=\u001b[39m max_object_length\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kg_triplet_extract_fn \u001b[38;5;241m=\u001b[39m kg_triplet_extract_fn\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# TODO: legacy conversion - remove in next release\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_struct\u001b[38;5;241m.\u001b[39mtable) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_store, SimpleGraphStore)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_store\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mgraph_dict) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     97\u001b[0m ):\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/base.py:73\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 73\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/base.py:179\u001b[0m, in \u001b[0;36mBaseIndex.build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docstore\u001b[38;5;241m.\u001b[39madd_documents(nodes, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/knowledge_graph/base.py:182\u001b[0m, in \u001b[0;36mKnowledgeGraphIndex._build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m triplet \u001b[38;5;129;01min\u001b[39;00m triplets:\n\u001b[1;32m    181\u001b[0m     subj, _, obj \u001b[38;5;241m=\u001b[39m triplet\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert_triplet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriplet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     index_struct\u001b[38;5;241m.\u001b[39madd_node([subj, obj], n)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_embeddings:\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/indices/knowledge_graph/base.py:230\u001b[0m, in \u001b[0;36mKnowledgeGraphIndex.upsert_triplet\u001b[0;34m(self, triplet)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupsert_triplet\u001b[39m(\u001b[38;5;28mself\u001b[39m, triplet: Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Insert triplets.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    Used for manual insertion of KG triplets (in the form\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert_triplet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtriplet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/graph_stores/kuzu.py:153\u001b[0m, in \u001b[0;36mKuzuGraphStore.upsert_triplet\u001b[0;34m(self, subj, rel, obj)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_rel\u001b[39m(connection: Any, subj: \u001b[38;5;28mstr\u001b[39m, obj: \u001b[38;5;28mstr\u001b[39m, rel: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     connection\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    144\u001b[0m         (\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMATCH (n1:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m), (n2:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) WHERE n1.ID = $subj AND n2.ID = $obj \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubj\u001b[39m\u001b[38;5;124m\"\u001b[39m, subj), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m, rel)],\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m is_subj_exists \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_entity_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m is_obj_exists \u001b[38;5;241m=\u001b[39m check_entity_exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnection, obj)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_subj_exists:\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/llama_index/graph_stores/kuzu.py:118\u001b[0m, in \u001b[0;36mKuzuGraphStore.upsert_triplet.<locals>.check_entity_exists\u001b[0;34m(connection, entity)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_entity_exists\u001b[39m(connection: Any, entity: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 118\u001b[0m     is_exists_result \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMATCH (n:\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m) WHERE n.ID = $entity RETURN n.ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_table_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_exists_result\u001b[38;5;241m.\u001b[39mhas_next()\n",
      "File \u001b[0;32m~/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/kuzu/connection.py:79\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, query, parameters)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(parameters) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# TODO(Chang): remove ROLLBACK once we can guarantee database is deleted after conn\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROLLBACK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m_prepared_statement, {})\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters must be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m prepared_statement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m     81\u001b[0m     query) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(query) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m query\n\u001b[1;32m     82\u001b[0m _query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m     83\u001b[0m                        prepared_statement\u001b[38;5;241m.\u001b[39m_prepared_statement,\n\u001b[1;32m     84\u001b[0m                        parameters)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parameters must be a dict"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True,\n",
    "    show_progress=True,\n",
    "    # kg_triplet_extract_fn=extract_triplets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75941ad5-f959-448f-8a5b-0d82283454f4",
   "metadata": {},
   "source": [
    "## 3. Create VectorStoreIndex for RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2fe8cd-ae1c-4f79-9b5d-8faf280e2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765529d-7661-4bdb-ad9f-1bf9eb9439de",
   "metadata": {},
   "source": [
    "## 4. (Optional) Persist and Load from disk Llama Indexes\n",
    "### 4.1. Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2f0d64c-c250-4f3e-8b90-8189f63f11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir='../data/storage_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92d72368-d7c2-4545-b7b8-97f31d3daa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index.storage_context.persist(persist_dir='../data/storage_vector')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751036f9-66a1-4424-b9a5-3c565af6278b",
   "metadata": {},
   "source": [
    "### 4.2. Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "991c3039-db1b-4170-aa3b-cdfc2c340215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='../data/storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='../data/storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d72ffb-6edf-4742-887d-0d488eb4d254",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f8a1b-30d5-4818-b0c0-dd88a53bfd43",
   "metadata": {},
   "source": [
    "### 5.1 Graph RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e09db-85c3-497a-988d-9d9310e0aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "kg_rag_query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48a6694e-aeb7-4ee9-a352-d70d5330e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_rag_query_engine = kg_index.as_query_engine(\n",
    "#     include_text=False,\n",
    "#     retriever_mode=\"keyword\",\n",
    "#     response_mode=\"tree_summarize\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8d77a-2e60-4064-ac8a-6cf07a33ebe0",
   "metadata": {},
   "source": [
    "### 5.2. Vector RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00213f39-24e3-4877-b02b-90ed33916e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca73d7-3d1f-406f-b742-1707987f9b1c",
   "metadata": {},
   "source": [
    "### 5.3 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a872670e-7165-41e2-a9a2-aa7a9bc8b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b207cc17-6c26-487b-8aaf-2f06509aa6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84ac10f5-ba9e-48ae-b579-de10517eb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d859e-4f54-4b76-821d-44f5349a744a",
   "metadata": {},
   "source": [
    "## 6. Query with the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "469c61b9-aa5a-41c6-b374-db678a160f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd65e4-bc57-4434-afc4-75232446ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Tell me about Peter Quill?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3724453-b2e3-48e2-9ac2-a5cc8a0f7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await query_engine.aquery(\n",
    "    \"Tell me about Peter Quill?\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "608ba3b1-1b7d-4818-ab81-a1bf33c34f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No relationships found, returning nodes found by keywords.\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No nodes found by keywords, returning empty response.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill, also known as Star-Lord, is a fictional character appearing in American comic books published by Marvel Comics. He was created by writer Dan Abnett and artist Andy Lanning, and first appeared in \"Annihilators\" #1 in July 2006.\n",
       "\n",
       "Peter Quill is a human from Earth, born in 1982. He was abducted from Earth when he was a young boy and raised among the Ravagers, an intergalactic pirate gang. Quill became a skilled mercenary and thief, using the alias Star-Lord. He gained prominence when he led a team of heroes known as the Guardians of the Galaxy to save the universe from various threats.\n",
       "\n",
       "Quill is known for his advanced weaponry, including the elementally powered Star-Lord gun and the Orb, an ancient relic that contains the power of a Celestial. He also has a sentient spaceship named the Milano, which can fly through space and time.\n",
       "\n",
       "Quill's personality is described as being charming, rebellious, and sarcastic, with a strong sense of loyalty to those he cares about. He has been romantically involved with several characters in the Marvel Universe, including Gamora and Rocket Raccoon.\n",
       "\n",
       "Quill made his cinematic debut in the 2014 film \"Guardians of the Galaxy,\" portrayed by Chris Pratt. The character has since appeared in two sequels and other Marvel Cinematic Universe films.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be8d4f82-64d8-4c91-b455-e83d56d83d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill, also known as Star-Lord, is a fictional character in the Marvel Cinematic Universe. He is portrayed by Chris Pratt in the films \"Guardians of the Galaxy\" (2014), \"Guardians of the Galaxy Vol. 2\" (2017), and \"Avengers: Infinity War\" and \"Avengers: Endgame\" (both 2018). Peter Quill is a human who was abducted from Earth as a child and grew up among extraterrestrial beings, becoming a space adventurer and eventually joining the titular Guardians of the Galaxy. He is known for his skill in combat and piloting spaceships, as well as his quirky sense of humor and love for classic Earth music. In the movies, he is depicted as a more traditional superhero compared to the Guardians, although not necessarily a hero. James Gunn, who directed the first two \"Guardians\" films, was fired by Disney in July 2018 over old tweets containing controversial remarks about rape and pedophilia, but there were efforts from Marvel and Disney to find a way for him to return to the franchise in some capacity. However, these negotiations ultimately failed and Gunn went on to work on other projects.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1692d8d-465f-4a21-b11f-9c3c437831f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No relationships found, returning nodes found by keywords.\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No nodes found by keywords, returning empty response.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill, also known as Star-Lord, is the leader of the Guardians of the Galaxy. He was abducted from Earth as a child and raised by a group of alien thieves and smugglers called the Ravagers. In the film \"Guardians of the Galaxy Vol. 3,\" Quill is in a state of depression following the appearance of an alternate version of his deceased lover Gamora, who does not share the same affection for him as her older version did, affecting his leadership of the Guardians. Chris Pratt portrays Peter Quill in the film.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23595b38-836f-4310-b54f-264b58e21dd6",
   "metadata": {},
   "source": [
    "## 7. Comparison and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "332a72a1-ac5c-4b32-9fac-034539ec46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = llm.complete(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acd90d24-8cd0-4ee8-81f5-07591118741e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " | Fact | Graph Result | Vector Result | Both Results |\n",
       "| --- | --- | --- | --- |\n",
       "| Character Name | Peter Quill / Star-Lord | Peter Quill / Star-Lord | Peter Quill / Star-Lord |\n",
       "| Universe | Marvel Comics | Marvel Cinematic Universe | Marvel Universe |\n",
       "| Creation | Created by Dan Abnett and Andy Lanning, first appeared in \"Annihilators\" #1 in July 2006 | Portrayed by Chris Pratt in films since 2014 | Created, first appearance in respective universes |\n",
       "| Species | Human from Earth | Human who was abducted from Earth as a child and grew up among extraterrestrial beings | Human, abducted from Earth as a child, raised among aliens |\n",
       "| Birth Year | Born in 1982 | N/A | Born in 1982 |\n",
       "| Backstory | Raised among the Ravagers, skilled mercenary and thief, leads Guardians of the Galaxy | Skilled in combat and piloting spaceships, quirky sense of humor, love for classic Earth music, depicted as more traditional superhero compared to the Guardians | Raised among the Ravagers, became a space adventurer, joined the Guardians of the Galaxy |\n",
       "| Weaponry | Star-Lord gun, Orb, Milano spaceship | N/A | Advanced weaponry, including Star-Lord gun and Orb, Milano spaceship |\n",
       "| Personality | Charming, rebellious, sarcastic, strong sense of loyalty to those he cares about, romantically involved with Gamora and Rocket Raccoon | Quirky sense of humor, love for classic Earth music, depicted as a more traditional superhero compared to the Guardians | Charming, rebellious, sarcastic, romantic relationships with Gamora and Rocket Raccoon |\n",
       "| Films | N/A | Appeared in \"Guardians of the Galaxy\" (2014), \"Guardians of the Galaxy Vol. 2\" (2017), \"Avengers: Infinity War\" (2018), and \"Avengers: Endgame\" (2018) | N/A, appears in films |\n",
       "| Director | N/A | James Gunn was fired from the franchise over old tweets, but negotiations to return ultimately failed | N/A, no mention of director |\n",
       "| Depiction in \"Vol. 3\" | In a state of depression following Gamora's appearance | N/A | In a state of depression following Gamora's appearance in \"Guardians of the Galaxy Vol. 3\" |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(analysis.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
