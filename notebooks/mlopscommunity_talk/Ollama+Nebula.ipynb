{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e6ca89-674d-4327-90d4-db1a5920a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f04381e-896a-4ee4-913d-07b23e5ee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac12660-412e-44d8-aafe-b83a1f0f5eab",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "### 1.1 Prepare LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60720ddb-2a55-4b97-a0de-e15be24a57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "OLLAMA_HOST = 'localhost'\n",
    "OLLAMA_MODEL = 'mistral'\n",
    "llm = Ollama(model=OLLAMA_MODEL, base_url=\"http://\"+OLLAMA_HOST+\":11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bb1371-01bf-40cb-b56a-58bc219ba532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/louis.guitton/workspace/mlops-talk-llm-kg/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    # To save costs, we use a local model.\n",
    "    # This will use a well-performing and fast default from Hugging Face.\n",
    "    # this model has dim of 384 https://huggingface.co/BAAI/bge-small-en\n",
    "    embed_model=\"local:BAAI/bge-small-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285d07a-8d12-4dee-b3cf-4fd1e1a863b5",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Graph Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f478476-a9c0-48d9-939a-22871caf4a6a",
   "metadata": {},
   "source": [
    "You need a running Nebula instance, you can start one with the Docker desktop Nebula Extension.\n",
    "Once you have Nebula running, you have a first-time setup \n",
    "```cypher\n",
    "ADD HOSTS \"storaged0\":9779,\"storaged1\":9779,\"storaged2\":9779\n",
    "```\n",
    "then you need to create the index before using it\n",
    "```cypher\n",
    "CREATE SPACE wikipedia(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    "```\n",
    "and\n",
    "```cypher\n",
    "USE wikipedia;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e8b3c3-20da-4c99-812e-6b2fa1b7e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.environ[\"NEBULA_USER\"] != \"\"\n",
    "assert os.environ[\"NEBULA_PASSWORD\"] != \"\"\n",
    "assert os.environ[\"NEBULA_ADDRESS\"] != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a2128b-9575-4a2b-bc0d-bfcdcde5a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=\"wikipedia\",\n",
    "    edge_types=[\"relationship\"],\n",
    "    rel_prop_names=[\"relationship\"],\n",
    "    tags=[\"entity\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751a5075-72d7-4fbd-99af-3ca75de8dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50bd077-5864-4fb7-8a25-793e9eaa69fd",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "### 2.1 Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a8e7d3-349b-4f37-8bcb-2ecebb819833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fcf49-16f5-47b5-a2a3-561fd25211ff",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to Graph\n",
    "reference:\n",
    "- [KnowledgeGraphIndex](https://docs.llamaindex.ai/en/stable/api_reference/indices/kg.html#llama_index.indices.knowledge_graph.KnowledgeGraphIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfd75ae9-d71a-4a6e-be19-ba38cbf9a6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.62it/s]\n",
      "Processing nodes:   0%|                                                                                                                                               | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.31it/s]\u001b[A\n",
      "Processing nodes:   6%|████████▍                                                                                                                              | 1/16 [00:07<01:52,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 27.74it/s]\u001b[A\n",
      "Processing nodes:  12%|████████████████▉                                                                                                                      | 2/16 [00:19<02:22, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                          | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 41.05it/s]\u001b[A\n",
      "Processing nodes:  19%|█████████████████████████▎                                                                                                             | 3/16 [00:31<02:23, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 25.81it/s]\u001b[A\n",
      "Processing nodes:  25%|█████████████████████████████████▊                                                                                                     | 4/16 [00:44<02:20, 11.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 42.98it/s]\u001b[A\n",
      "Processing nodes:  31%|██████████████████████████████████████████▏                                                                                            | 5/16 [00:49<01:43,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 40.49it/s]\u001b[A\n",
      "Processing nodes:  38%|██████████████████████████████████████████████████▋                                                                                    | 6/16 [00:57<01:28,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 80.82it/s]\u001b[A\n",
      "Processing nodes:  44%|███████████████████████████████████████████████████████████                                                                            | 7/16 [01:04<01:13,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18.92it/s]\u001b[A\n",
      "Processing nodes:  50%|███████████████████████████████████████████████████████████████████▌                                                                   | 8/16 [01:09<00:57,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 77.62it/s]\u001b[A\n",
      "Processing nodes:  56%|███████████████████████████████████████████████████████████████████████████▉                                                           | 9/16 [01:27<01:13, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 43.01it/s]\u001b[A\n",
      "Processing nodes:  62%|███████████████████████████████████████████████████████████████████████████████████▊                                                  | 10/16 [01:36<01:00, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 54.92it/s]\u001b[A\n",
      "Processing nodes:  69%|████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 11/16 [01:47<00:52, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 67.60it/s]\u001b[A\n",
      "Processing nodes:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                 | 12/16 [02:02<00:47, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 64.14it/s]\u001b[A\n",
      "Processing nodes:  81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 13/16 [02:10<00:31, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 52.93it/s]\u001b[A\n",
      "Processing nodes:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 14/16 [02:17<00:19,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings:   0%|                                                                                                                                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Generating embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.32it/s]\u001b[A\n",
      "Processing nodes:  94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 15/16 [02:32<00:11, 11.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings: 0it [00:00, ?it/s]\u001b[A\n",
      "Processing nodes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [02:40<00:00, 10.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import KnowledgeGraphIndex\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=5,\n",
    "    include_embeddings=True,\n",
    "    show_progress=True,\n",
    "    # max_object_length: int = 128,\n",
    "    space_name=\"wikipedia\",\n",
    "    edge_types=[\"relationship\"],\n",
    "    rel_prop_names=[\"relationship\"],\n",
    "    tags=[\"entity\"],\n",
    "    # to extract triplets, kg_triplet_extract_fn is used if not None,\n",
    "    # kg_triplet_extract_fn: Optional[Callable] = None, \n",
    "    # else, the LLM from the service context is used with the kg_triple_extract_template if not None else the default triplet extract prompt\n",
    "    # kg_triple_extract_template: Optional[BasePromptTemplate] = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44c81d-8dd9-4a7c-a673-17f6362dae64",
   "metadata": {},
   "source": [
    "### Conclusions from building the KG\n",
    "\n",
    "- we can visualize the graph in Neo4j Bloom directly on top of Neo4j local or Neo4j AuraDB\n",
    "- we have 1 Node type: Entity\n",
    "    ```cypher\n",
    "    MATCH (n)\n",
    "    RETURN distinct labels(n)[0] as label, count(n) as node_count\n",
    "    ```\n",
    "- 80 Relationship type\n",
    "    ```cypher\n",
    "    MATCH p=()-->() RETURN count(p)\n",
    "    ```\n",
    "- Entities only have 1 field called `id`, we don't have entity type like \"Person\" etc... NER would be needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a23fd7-8795-4bc0-8eb2-92ceddd94dd9",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a13261-1337-4b40-b120-a71bdc457142",
   "metadata": {},
   "source": [
    "### 5.1 Graph RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a337c6-aa13-4e4f-b19a-1a89560f0288",
   "metadata": {},
   "source": [
    "There are issues with the llama_index docs as I describe [here](https://github.com/run-llama/llama_index/issues/10474)\n",
    "\n",
    "reference:\n",
    "\n",
    "- [KGTableRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/kg.html#llama_index.indices.knowledge_graph.retrievers.KGTableRetriever) ??\n",
    "- [KnowledgeGraphRAGRetriever](https://docs.llamaindex.ai/en/stable/api_reference/query/retrievers/kg.html#llama_index.indices.knowledge_graph.retrievers.KnowledgeGraphRAGRetriever) \"Retriever that perform SubGraph RAG towards knowledge graph.\"\n",
    "- [RetrieverQueryEngine](https://docs.llamaindex.ai/en/stable/api_reference/query/query_engines/retriever_query_engine.html)\n",
    "\n",
    "Explanation on the different Retrievers in the [docs](https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html#why-knowledge-graph-rag-query-engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d3bfebc-87f9-4d36-be7c-fbc0bf5fd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we just built the index and we have it available\n",
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False, \n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdfe46f6-19d6-489c-ae12-960151451cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.knowledge_graph.retrievers import KGTableRetriever\n",
    "\n",
    "assert type(kg_rag_query_engine.retriever) == KGTableRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d01d98-76d5-45a3-984f-40fe3c43f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag__embed_query_engine = kg_index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a67ed77-7dba-4d68-903f-d14574991dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "kg_rag_fancy_retriever_query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever, service_context=service_context\n",
    ")\n",
    "\n",
    "assert type(kg_rag_fancy_retriever_query_engine.retriever) == KnowledgeGraphRAGRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cce79-aada-41ef-9713-fe6558e72ae1",
   "metadata": {},
   "source": [
    "## 6. Query with the engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ee7aceb-bd1f-4aed-8116-1a18b2fbf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d147e85f-961e-4276-9a5b-305d353acb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "WARNING:llama_index.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill, also known as Star-Lord, is an entity who has been involved in several relationships. One of these relationships is described as \"Speaks uncensored\" with an object referred to as \"Fuck in vol. 3\". Another relationship is labeled as \"Reunites with\" and connects him to an entity named \"Grandfather on earth\".</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "273b3b68-dee8-4535-a1ef-8d28d67e3e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 032c7ecb-a63f-4ead-a22f-5a605141e376: == Plot ==\n",
      "At their new headquarters on Knowhere, the Guardians of the Galaxy...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: 16f10ef6-3cbb-4e95-aa5a-69fdd2ccd244: === Filming ===\n",
      "Principal photography began on November 8, 2021, at Trilith S...\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> Querying with idx: dd662463-9d50-4be7-b44f-a17cde4cb032: == Cast ==\n",
      "Chris Pratt as Peter Quill / Star-Lord:The half-human, half-Celest...\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill, also known as Star-Lord, is a half-human, half-Celestial character who was abducted from Earth as a child and raised by a group of alien thieves and smugglers called the Ravagers. He currently serves as the leader of the Guardians of the Galaxy. In the film, Quill is depicted in a \"state of depression\" due to the appearance of an alternate version of his dead lover Gamora who does not share the same affection for him. The role of Peter Quill is played by Chris Pratt. Quill speaks uncensored profanity, specifically the word 'fuck', in the film. Vin Diesel voices the character Groot, who is a member and accomplice of the Guardians. Quill decides to leave the Guardians and reunites with his grandfather on Earth.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag__embed_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32cb62c4-0b75-4619-8ab1-5f4bd670c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b> Peter Quill is an individual involved in various film projects. He has been identified as speaking uncensored language in several films, including \"Film vol. 3\" and the \"Guardians of the galaxy holiday special.\" In relation to \"Film vol. 3,\" Quill serves multiple roles such as cinematographer for Henry Braham and costume designer for Judianna Makovsky. Additionally, Quill decided to leave his guardians and reunited with his grandfather on earth. Industrial light & magic's stagecraft virtual production technology was utilized in the filming of \"Film vol. 3.\" Chukwudi Iwuji is also part of this project as revealed by Quill.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_fancy_retriever_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e59a34-3a15-4bc6-a48c-bafb01d11970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
